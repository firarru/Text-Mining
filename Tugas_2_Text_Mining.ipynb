{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPWXspUhLH8KW4QTuE/jlvr",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/firarru/Text-Mining/blob/main/Tugas_2_Text_Mining.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Import Package**"
      ],
      "metadata": {
        "id": "h8LIbz5xTdz_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dWe98omOSCSw"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk import FreqDist\n",
        "from sklearn.feature_extraction.text import CountVectorizer"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Scraping Text**"
      ],
      "metadata": {
        "id": "yZNP0ui2UMRX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "url = 'https://www.koreatimes.co.kr/www/art/2025/03/398_393519.html'\n",
        "\n",
        "response = requests.get(url)\n",
        "soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "paragraphs = soup.find_all('p')\n",
        "\n",
        "artikel = ''\n",
        "\n",
        "for paragraph in paragraphs:\n",
        "    artikel += paragraph.get_text() + '\\n'\n",
        "\n",
        "print(artikel)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XA8CKVw3ULFq",
        "outputId": "1d9212ff-746c-4b9f-bc00-052b29f522a4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1\n",
            "Number of abandoned homes reaches 1.53 mil. in 2023 amid aging population \n",
            "3\n",
            "15 injured after KF-16 fighter jets mistakenly drop bombs outside training range \n",
            "5\n",
            "'When Life Gives You Tangerines' is story of life, love, melted into 4 seasons of Jeju Island \n",
            "7\n",
            "Trump's remark of 'South Korea's 4 times higher tariffs' inaccurate: trade ministry \n",
            "9\n",
            "Stronger yen could dampen Koreans' travel frenzy to Japan \n",
            "11\n",
            "Trump claims Korea's average tariff is 4 times higher though US helps Korea in 'so many ways' \n",
            "13\n",
            "Seventeen’s Wonwoo to enlist on April 3\n",
            "15\n",
            "Gov't to disclose results of probe into forced overseas adoptions this month \n",
            "17\n",
            "Will English-only rule take root in a Korean company this time? \n",
            "19\n",
            "Rise of deepfakes in entertainment ignites both excitement, concerns \n",
            "2\n",
            "Antidumping tariffs on Chinese steel products draw backlash \n",
            "4\n",
            "Ador files court complaint to restrict NewJeans' (NJZ) music activities \n",
            "6\n",
            "Brandy Melville's first Korean store sparks concern over lack of size inclusivity \n",
            "8\n",
            "Actor Kim Nam-gil wraps up five-hour fan meet \n",
            "10\n",
            "Korea to introduce 'top-tier visa' to attract tech experts \n",
            "12\n",
            "N. Korea abruptly suspends resumed foreign tours to Rason \n",
            "14\n",
            "Korea urged to grant residency to undocumented migrant children \n",
            "16\n",
            "Korean economy grows 2% in 2024; per capita income surpasses Japan: BOK \n",
            "18\n",
            "Local governments step up efforts to deal with youth job crisis \n",
            "20\n",
            "New allegations escalate NJZ-Ador battle ahead of court hearing \n",
            "Close for 24 hours\n",
            "Seventeen’s Wonwoo enlists on April 3 / Korea Times file\n",
            "K-pop boy group Seventeen’s Wonwoo is set to enlist in the military, becoming the second member of the group to begin his service following Jeonghan.\n",
            "His agency, Pledis Entertainment, said on Wednesday, \"Wonwoo will begin fulfilling his mandatory military service on April 3.\"\n",
            "As a result, Wonwoo will not participate in scheduled activities after his enlistment, including the \"Tecate Pa'l Norte 2025\" festival and Seventeen’s Japan fan meet. \n",
            "However, the agency reassured fans, saying, \"He will take part in the ‘Carat Land’ concerts on March 20–21 as planned and will continue engaging with fans through preprepared content.\"\n",
            "There will be no official event on the day of his enlistment. \"We ask fans to refrain from visiting his alternative service site,\" the agency added. \"Please continue to support Wonwoo so he can complete his service diligently and return in good health.\"\n",
            "This article from the Hankook Ilbo, the sister publication of The Korea Times, is translated by a generative AI system and edited by The Korea Times.\n",
            "Bayern Munich beat Tottenham Hotspur in Seoul \n",
            "Tottenham 4-3 K-League All-Stars \n",
            "World Water Day 2024 \n",
            "Busan World Team Table Tennis Championships Finals \n",
            "Super Bowl 2024 \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Case Folding**"
      ],
      "metadata": {
        "id": "NyHU7_0IWJAr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "artikel = artikel.lower()\n",
        "print(artikel)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EaIDJjCUWBWI",
        "outputId": "1a444495-c973-4253-f004-9f9bd45b8c0f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "artikel = re.sub(r'\\d+', '', artikel)\n",
        "artikel_cf = re.sub(r'[^\\w\\s]', '', artikel)\n",
        "print(artikel_cf)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wh4J0j7SWQ91",
        "outputId": "47acb6ad-d803-42e6-a7fe-7a4ccaf812cc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Tokenizing**"
      ],
      "metadata": {
        "id": "JCujiKbtZ92m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt_tab')\n",
        "token_artikel = word_tokenize(artikel_cf)\n",
        "print(token_artikel)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vtoQCYAEXTB1",
        "outputId": "86d0133b-68a0-49b3-b2ad-850d6e45ef4a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "token_artikel = artikel_cf.split()\n",
        "print(token_artikel)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4OcexDLGaFm0",
        "outputId": "3b0af6c3-aad8-41cc-9589-06d451b6d0f9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')\n",
        "stop_words = set(stopwords.words('english'))\n",
        "filtered_tokens = [word for word in token_artikel if word.casefold() not in stop_words]\n",
        "print(filtered_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sUORu512aih6",
        "outputId": "332c944a-eedb-47c6-e15d-c0606ff963fe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fdist = FreqDist(filtered_tokens)\n",
        "print(fdist)\n",
        "print(fdist.most_common(15))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fXfiHa5aartU",
        "outputId": "c7fb9aed-106b-4f0a-921f-ec1a3ad81c68"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<FreqDist with 0 samples and 0 outcomes>\n",
            "[]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install sastrawi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G4MViBw_eR-a",
        "outputId": "6ab9da6e-0f10-466d-9bb3-ebf0197b6a50"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting sastrawi\n",
            "  Downloading Sastrawi-1.0.1-py2.py3-none-any.whl.metadata (909 bytes)\n",
            "Downloading Sastrawi-1.0.1-py2.py3-none-any.whl (209 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/209.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.7/209.7 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: sastrawi\n",
            "Successfully installed sastrawi-1.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory\n",
        "\n",
        "factory = StopWordRemoverFactory()\n",
        "stopword = factory.create_stop_word_remover()\n",
        "\n",
        "filtered_tokens = [stopword.remove(word) for word in token_artikel]\n",
        "print(filtered_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bDQu2bSpecFQ",
        "outputId": "63257639-92fd-40c3-a4a3-3a4600f83fc8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
        "\n",
        "factory = StemmerFactory()\n",
        "stemmer = factory.create_stemmer()\n",
        "\n",
        "stemmed_tokens = [stemmer.stem(word) for word in filtered_tokens]\n",
        "print(stemmed_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nb9c5WlLg3Af",
        "outputId": "25d2712b-fd20-49a5-eb01-594288bc3f3f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade scikit-learn"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NkOXJCbtgYTz",
        "outputId": "9c107bea-18d7-4b55-cc0a-6d4908b44f71"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.5.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "vectorizer = TfidfVectorizer()\n",
        "\n",
        "tfidf = vectorizer.fit_transform(stemmed_tokens)\n",
        "print(\"Feature name :\", vectorizer.get_feature_names_out())\n",
        "print(\"TF-IDF Vector :\", tfidf.toarray())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 369
        },
        "id": "J_MLKvdjf17I",
        "outputId": "e310384a-8b83-4449-cd56-505cf060b26a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "empty vocabulary; perhaps the documents only contain stop words",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-25bb214b67e5>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mvectorizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTfidfVectorizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mtfidf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstemmed_tokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Feature name :\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_feature_names_out\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"TF-IDF Vector :\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtfidf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   2102\u001b[0m             \u001b[0msublinear_tf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msublinear_tf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2103\u001b[0m         )\n\u001b[0;32m-> 2104\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_documents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2105\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tfidf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2106\u001b[0m         \u001b[0;31m# X is already a transformed view of raw_documents so\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1387\u001b[0m                 )\n\u001b[1;32m   1388\u001b[0m             ):\n\u001b[0;32m-> 1389\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfit_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1390\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1391\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   1374\u001b[0m                     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1375\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1376\u001b[0;31m         \u001b[0mvocabulary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_count_vocab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_documents\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfixed_vocabulary_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1377\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1378\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36m_count_vocab\u001b[0;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[1;32m   1280\u001b[0m             \u001b[0mvocabulary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocabulary\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1281\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1282\u001b[0;31m                 raise ValueError(\n\u001b[0m\u001b[1;32m   1283\u001b[0m                     \u001b[0;34m\"empty vocabulary; perhaps the documents only contain stop words\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1284\u001b[0m                 )\n",
            "\u001b[0;31mValueError\u001b[0m: empty vocabulary; perhaps the documents only contain stop words"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "x2uvbBvKgKoR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}